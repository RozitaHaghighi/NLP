{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install wikipedia-api"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qjBblmsqru4x",
        "outputId": "0242cef6-9d50-420f-f4bf-1cb1bf4929a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wikipedia-api\n",
            "  Downloading wikipedia_api-0.7.1.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from wikipedia-api) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (2024.8.30)\n",
            "Building wheels for collected packages: wikipedia-api\n",
            "  Building wheel for wikipedia-api (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia-api: filename=Wikipedia_API-0.7.1-py3-none-any.whl size=14346 sha256=0100118819949233148f1287155d015505d4c854443898e4be21067b3ba94cca\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/96/18/b9201cc3e8b47b02b510460210cfd832ccf10c0c4dd0522962\n",
            "Successfully built wikipedia-api\n",
            "Installing collected packages: wikipedia-api\n",
            "Successfully installed wikipedia-api-0.7.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wikipediaapi\n",
        "\n",
        "# Initialize Wikipedia API with a custom user agent\n",
        "wiki_wiki = wikipediaapi.Wikipedia(\n",
        "    language='en',\n",
        "    user_agent='CBOWModel/1.0 (https://yourdomain.com; contact@example.com)'\n",
        ")\n",
        "\n",
        "# Function to extract text from a Wikipedia page\n",
        "def get_wikipedia_content(page_title):\n",
        "    page = wiki_wiki.page(page_title)\n",
        "    if page.exists():\n",
        "        return page.text\n",
        "    else:\n",
        "        print(f\"Page '{page_title}' not found.\")\n",
        "        return \"\"\n",
        "\n",
        "# List of relevant pages for states and capitals\n",
        "pages = [\"List of U.S. state capitals\", \"Geography of the United States\"]\n",
        "\n",
        "# Collect the content into one corpus\n",
        "corpus = \"\"\n",
        "for page in pages:\n",
        "    corpus += get_wikipedia_content(page) + \"\\n\"\n",
        "\n",
        "# Save the corpus to a text file\n",
        "with open(\"us_states_capitals_corpus.txt\", \"w\") as f:\n",
        "    f.write(corpus)\n",
        "\n",
        "print(\"Corpus saved successfully!\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LnLssfB5rWLq",
        "outputId": "0583a0c3-dc21-4304-b3cb-b9f5dbce32c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corpus saved successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from random import sample  # Ensure this is imported\n",
        "\n",
        "# Load your custom Wikipedia text corpus\n",
        "def load_custom_corpus(file_path):\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        corpus = f.read()\n",
        "    return corpus\n",
        "\n",
        "# Use your corpus instead of loading from datasets\n",
        "corpus_file = \"us_states_capitals_corpus.txt\"\n",
        "corpus = load_custom_corpus(corpus_file)  # Corrected: Passing the variable\n",
        "\n",
        "# Optional: Split the corpus into smaller chunks if needed\n",
        "def sample_corpus(corpus, num_samples=5000):\n",
        "    lines = corpus.split(\"\\n\")\n",
        "    return \"\\n\".join(sample(lines, min(num_samples, len(lines))))\n",
        "\n",
        "# Get a trimmed version of the corpus with 5000 lines (or fewer if the corpus is smaller)\n",
        "trimmed_corpus = sample_corpus(corpus, num_samples=5000)\n",
        "\n",
        "# Confirm the trimmed corpus length\n",
        "print(f\"Trimmed Corpus Length: {len(trimmed_corpus.splitlines())} lines\")\n",
        "\n",
        "# Now you can proceed to use `trimmed_corpus` in your CBOW training or any other tasks\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pADiPWcLEKcd",
        "outputId": "37d3c398-dd68-4674-ae13-71df11d69001"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trimmed Corpus Length: 276 lines\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "# 1. Load the corpus (assuming 'trimmed_corpus' has the preloaded content)\n",
        "text = trimmed_corpus.lower()  # Convert text to lowercase\n",
        "\n",
        "# 2. Remove special characters, punctuation, and digits using regex\n",
        "text = re.sub(r'[^a-z\\s]', '', text)  # Keep only letters and spaces\n",
        "\n",
        "# 3. Tokenize the text into words (split by whitespace)\n",
        "tokens = text.split()\n",
        "\n",
        "# 4. Build vocabulary with word-to-index and index-to-word mappings\n",
        "vocab = Counter(tokens)  # Count frequency of each word\n",
        "\n",
        "# Optional: Limit vocabulary size (if needed) to most common words\n",
        "# vocab = vocab.most_common(10000)  # Uncomment this line if you want a capped vocab size\n",
        "\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# 5. Create word-to-index and index-to-word dictionaries\n",
        "word_to_index = {word: idx for idx, (word, _) in enumerate(vocab.items())}\n",
        "index_to_word = {idx: word for word, idx in word_to_index.items()}\n",
        "\n",
        "# 6. Print vocabulary size and sample of the mappings\n",
        "print(f\"Vocabulary Size: {vocab_size}\")\n",
        "print(\"Sample word-to-index mappings:\", list(word_to_index.items())[:10])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zwFh4DK_G_C8",
        "outputId": "448e0249-d39d-4bb9-89d3-584380245578"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary Size: 1643\n",
            "Sample word-to-index mappings: [('hilo', 0), ('references', 1), ('an', 2), ('insular', 3), ('area', 4), ('is', 5), ('a', 6), ('united', 7), ('states', 8), ('territory', 9)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "def vocab_frequency(text):\n",
        "    \"\"\"\n",
        "    Creates a dictionary of word frequencies based on a dataset.\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    text : String\n",
        "        Preprocessed text in which words will be counted.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    vocab_dict : dictionary\n",
        "        Dictionary of words and their frequencies with the format {word: frequency}.\n",
        "    \"\"\"\n",
        "    vocab_dict = {}\n",
        "\n",
        "    # Split the text into words\n",
        "    words = text.split()\n",
        "\n",
        "    # Count the frequency of each word\n",
        "    for word in words:\n",
        "        if word in vocab_dict:\n",
        "            vocab_dict[word] += 1\n",
        "        else:\n",
        "            vocab_dict[word] = 1\n",
        "\n",
        "    return vocab_dict\n",
        "\n",
        "# Step 1: Preprocess and generate the vocabulary frequency dictionary from trimmed corpus\n",
        "# Lowercase and clean the corpus to keep only alphabets and spaces\n",
        "cleaned_text = re.sub(r'[^a-z\\s]', '', trimmed_corpus.lower())\n",
        "\n",
        "# Generate the vocabulary dictionary\n",
        "vocabulary = vocab_frequency(cleaned_text)\n",
        "\n",
        "# Step 2: Print vocabulary size\n",
        "print(f\"Vocabulary Size: {len(vocabulary)}\")\n",
        "\n",
        "# Step 3: Print top 10 most common words\n",
        "sorted_vocab = sorted(vocabulary.items(), key=lambda x: x[1], reverse=True)\n",
        "print(\"Top 10 most common words and their frequencies:\", sorted_vocab[:10])\n",
        "\n",
        "# Step 4: Optional - Check the frequency of specific words\n",
        "print(f\"Frequency of 'state': {vocabulary.get('state', 0)}\")\n",
        "print(f\"Frequency of 'capital': {vocabulary.get('capital', 0)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8k_OawOLIOV4",
        "outputId": "f7db5fc1-4dc1-4cb6-d87e-3f6517ebd839"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary Size: 1643\n",
            "Top 10 most common words and their frequencies: [('the', 786), ('of', 361), ('and', 244), ('in', 207), ('to', 112), ('states', 108), ('a', 101), ('united', 82), ('is', 73), ('as', 73)]\n",
            "Frequency of 'state': 47\n",
            "Frequency of 'capital': 47\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from collections import Counter\n",
        "\n",
        "# Step 1: Generate the vocabulary from your corpus text\n",
        "# Assuming 'text' contains the trimmed corpus\n",
        "tokens = text.split()\n",
        "vocab = Counter(tokens)\n",
        "\n",
        "# Step 2: Create word-to-index mapping\n",
        "def create_word_to_index(vocabulary):\n",
        "    \"\"\"\n",
        "    Generates a word-to-index mapping from the vocabulary.\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    vocabulary : dict\n",
        "        Dictionary containing words and their frequencies.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    word_to_index : dict\n",
        "        Mapping of words to indices.\n",
        "    \"\"\"\n",
        "    word_to_idx = {'<OOV>': 0}  # Reserve 0 for OOV words\n",
        "    for idx, word in enumerate(vocabulary.keys(), start=1):\n",
        "        word_to_idx[word] = idx\n",
        "    return word_to_idx\n",
        "\n",
        "# Step 3: Generate the word-to-index mapping\n",
        "word_to_index_mapping = create_word_to_index(vocab)\n",
        "\n",
        "# Step 4: Verify the generated mapping\n",
        "print(\"Sample of word-to-index mapping:\")\n",
        "for word, index in list(word_to_index_mapping.items())[:10]:\n",
        "    print(f\"{word}: {index}\")\n",
        "\n",
        "# Step 5: Initialize your CBOW model with the correct vocabulary size\n",
        "vocab_size = len(word_to_index_mapping)  # Use the correct length of the mapping\n",
        "model = CBOW(vocab_size).to(device)  # Move the model to the appropriate device\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9EHVrU2IfNo",
        "outputId": "2db8660e-6bd7-4f39-c1ab-0311eedef662"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample of word-to-index mapping:\n",
            "<OOV>: 0\n",
            "hilo,: 1\n",
            "1796–1803: 2\n",
            "references: 3\n",
            "an: 4\n",
            "insular: 5\n",
            "area: 6\n",
            "is: 7\n",
            "a: 8\n",
            "united: 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def generate_dataset(data, window_size, word_to_index):\n",
        "    \"\"\"\n",
        "    Method to generate training dataset for CBOW.\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    data : String\n",
        "        Training dataset (a single string).\n",
        "    window_size : int\n",
        "        Size of the context window (number of words before and after the target).\n",
        "    word_to_index : Dictionary\n",
        "        Dictionary mapping words to indices with format {word: index}.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    surroundings : Tensor\n",
        "        Tensor with indices of surrounding words, shape (N x 2*window_size), where N is the number of samples.\n",
        "    targets : Tensor\n",
        "        Tensor with indices of target words, shape (N,).\n",
        "    \"\"\"\n",
        "\n",
        "    surroundings = []\n",
        "    targets = []\n",
        "\n",
        "    # Split the input data into a list of words\n",
        "    data = data.split()\n",
        "\n",
        "    # Iterate through the dataset starting at window_size and stopping window_size before the end\n",
        "    for i in range(window_size, len(data) - window_size):\n",
        "        # Collect the surrounding words, window_size on both sides of the target word\n",
        "        surrounding = data[i - window_size:i] + data[i + 1:i + 1 + window_size]\n",
        "\n",
        "        # Convert the surrounding words to their indices (use <OOV> if word not found)\n",
        "        surrounding_indices = [word_to_index.get(word, word_to_index['<OOV>']) for word in surrounding]\n",
        "\n",
        "        # Get the target word (in the middle) and convert it to its index\n",
        "        target_word = data[i]\n",
        "        target_index = word_to_index.get(target_word, word_to_index['<OOV>'])\n",
        "\n",
        "        # Append the surrounding indices and target index to their respective lists\n",
        "        surroundings.append(surrounding_indices)\n",
        "        targets.append(target_index)\n",
        "\n",
        "    # Convert lists to tensors for PyTorch compatibility\n",
        "    surroundings_tensor = torch.tensor(surroundings, dtype=torch.long)\n",
        "    targets_tensor = torch.tensor(targets, dtype=torch.long)\n",
        "\n",
        "    return surroundings_tensor, targets_tensor\n",
        "\n",
        "# Ensure the preprocessed text is being used\n",
        "text = trimmed_corpus.lower()  # Use the trimmed, cleaned corpus\n",
        "\n",
        "# Generate the dataset using window size of 2\n",
        "surroundings, targets = generate_dataset(text, window_size=2, word_to_index=word_to_index_mapping)\n",
        "\n",
        "# Display the shapes of the tensors\n",
        "print(f\"Surroundings Tensor Shape: {surroundings.shape}\")\n",
        "print(f\"Targets Tensor Shape: {targets.shape}\")\n",
        "\n",
        "# Display a sample of the data\n",
        "print(\"Sample data:\")\n",
        "print(f\"Surroundings (first 5): {surroundings[:5]}\")\n",
        "print(f\"Targets (first 5): {targets[:5]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WYt0SNhYI3Od",
        "outputId": "3efcf1d2-867e-406c-af5b-562dec45b5f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Surroundings Tensor Shape: torch.Size([7487, 4])\n",
            "Targets Tensor Shape: torch.Size([7487])\n",
            "Sample data:\n",
            "Surroundings (first 5): tensor([[1, 2, 4, 5],\n",
            "        [2, 3, 5, 6],\n",
            "        [3, 4, 6, 7],\n",
            "        [4, 5, 7, 8],\n",
            "        [5, 6, 8, 9]])\n",
            "Targets (first 5): tensor([3, 4, 5, 6, 7])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class CBOW(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim=300):\n",
        "        \"\"\"\n",
        "        Class to define the CBOW model.\n",
        "\n",
        "        Attributes\n",
        "        ----------\n",
        "        vocab_size : int\n",
        "            Size of the vocabulary.\n",
        "        embed_dim : int\n",
        "            Size of the embedding layer (dimension of word vectors).\n",
        "        \"\"\"\n",
        "        super(CBOW, self).__init__()\n",
        "\n",
        "        # Embedding layer: maps word indices to dense vectors of size embed_dim\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "\n",
        "        # Linear layer: projects averaged embedding to the vocabulary space\n",
        "        self.linear = nn.Linear(embed_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass through the network.\n",
        "\n",
        "        Arguments\n",
        "        ---------\n",
        "        x : Tensor\n",
        "            Input tensor containing indices of the surrounding words.\n",
        "            Shape: (batch_size, context_window * 2)\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        out : Tensor\n",
        "            Output tensor representing the predicted scores for each word in the vocabulary.\n",
        "            Shape: (batch_size, vocab_size)\n",
        "        \"\"\"\n",
        "\n",
        "        # Pass input through embedding layer (output shape: (batch_size, context_window * 2, embed_dim))\n",
        "        emb = self.embedding(x)\n",
        "\n",
        "        # Compute the average of the embeddings (shape: (batch_size, embed_dim))\n",
        "        average = torch.mean(emb, dim=1)\n",
        "\n",
        "        # Normalize the averaged embedding (L2 normalization)\n",
        "        normalized_average = average / average.norm(dim=1, keepdim=True)\n",
        "\n",
        "        # Pass through the linear layer (shape: (batch_size, vocab_size))\n",
        "        out = self.linear(normalized_average)\n",
        "\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "AIuAW47xJKCi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Create DataLoader for training data\n",
        "batch_size = 64  # Adjust this based on your GPU's capacity\n",
        "train_dataloader = DataLoader(\n",
        "    list(zip(surroundings, targets)), batch_size=batch_size, shuffle=True\n",
        ")"
      ],
      "metadata": {
        "id": "mEUCF5W2JP2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Set the device to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Ensure 'word_to_index_mapping' is the correct dictionary from your vocabulary\n",
        "vocab_size = len(word_to_index_mapping)  # Use the correct variable here\n",
        "\n",
        "# Initialize the CBOW model, loss function, and optimizer\n",
        "model = CBOW(vocab_size).to(device)  # Move model to device (GPU/CPU)\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Create TensorDataset and DataLoader from surroundings and targets\n",
        "train_dataset = TensorDataset(surroundings, targets)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "# Training loop\n",
        "epochs = 50\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0  # Initialize total loss for the epoch\n",
        "    model.train()  # Set the model to training mode\n",
        "\n",
        "    # Iterate over batches from DataLoader\n",
        "    for surr, tar in tqdm(train_dataloader):\n",
        "        # Move data to the appropriate device (GPU or CPU)\n",
        "        surr, tar = surr.to(device), tar.to(device)\n",
        "\n",
        "        # Clear gradients for the optimizer\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass: Get predictions from the model\n",
        "        log_probs = model(surr)\n",
        "\n",
        "        # Compute the loss between predicted output and actual target\n",
        "        loss = loss_function(log_probs, tar)\n",
        "\n",
        "        # Add the loss to the running total loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Backward pass: compute gradients\n",
        "        loss.backward()\n",
        "\n",
        "        # Update the model parameters\n",
        "        optimizer.step()\n",
        "\n",
        "    # Print the average loss for the epoch\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(train_dataloader)}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QtGBDTykJVeN",
        "outputId": "91359ba7-8c33-4bc8-a21c-66d6e7c43beb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 117/117 [00:05<00:00, 20.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50, Loss: 7.618497192350208\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 117/117 [00:04<00:00, 25.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/50, Loss: 7.305544930645543\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 117/117 [00:03<00:00, 37.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/50, Loss: 6.981784164396107\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 117/117 [00:02<00:00, 43.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/50, Loss: 6.64638341186393\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 117/117 [00:02<00:00, 45.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/50, Loss: 6.3549490219507465\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 117/117 [00:02<00:00, 44.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/50, Loss: 6.143565565092951\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 117/117 [00:02<00:00, 44.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/50, Loss: 5.988970173729791\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 117/117 [00:03<00:00, 35.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/50, Loss: 5.85591041124784\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 117/117 [00:02<00:00, 45.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/50, Loss: 5.729706218100001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 117/117 [00:02<00:00, 45.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/50, Loss: 5.603301830780812\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 117/117 [00:02<00:00, 46.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/50, Loss: 5.474025498088609\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 117/117 [00:02<00:00, 40.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12/50, Loss: 5.3414554881234455\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 117/117 [00:03<00:00, 37.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13/50, Loss: 5.204883449097984\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 117/117 [00:02<00:00, 47.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14/50, Loss: 5.066247614020975\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 117/117 [00:02<00:00, 44.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15/50, Loss: 4.926659180567815\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 117/117 [00:02<00:00, 46.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16/50, Loss: 4.788149915189824\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 117/117 [00:03<00:00, 38.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17/50, Loss: 4.651538865179078\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 117/117 [00:02<00:00, 39.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18/50, Loss: 4.517816500786023\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 117/117 [00:02<00:00, 45.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19/50, Loss: 4.386479316613613\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 117/117 [00:02<00:00, 45.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20/50, Loss: 4.258298568236522\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 117/117 [00:02<00:00, 46.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 21/50, Loss: 4.132192813433134\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 117/117 [00:03<00:00, 36.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 22/50, Loss: 4.008857743352906\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 117/117 [00:02<00:00, 42.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 23/50, Loss: 3.887326917077741\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 117/117 [00:02<00:00, 48.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 24/50, Loss: 3.7677288585238986\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 117/117 [00:02<00:00, 44.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 25/50, Loss: 3.64978164077824\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 117/117 [00:02<00:00, 44.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 26/50, Loss: 3.533448005333925\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 117/117 [00:03<00:00, 34.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 27/50, Loss: 3.418511123738737\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 117/117 [00:02<00:00, 45.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 28/50, Loss: 3.3051590776851034\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 117/117 [00:02<00:00, 44.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 29/50, Loss: 3.1931765120253606\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 117/117 [00:02<00:00, 45.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 30/50, Loss: 3.0826359977070084\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 117/117 [00:02<00:00, 41.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 31/50, Loss: 2.9734900649796185\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 117/117 [00:03<00:00, 37.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 32/50, Loss: 2.866074093386658\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 117/117 [00:02<00:00, 41.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 33/50, Loss: 2.7603948034791865\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 117/117 [00:03<00:00, 38.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 34/50, Loss: 2.655978247650668\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 117/117 [00:02<00:00, 46.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 35/50, Loss: 2.5538865959542427\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 117/117 [00:03<00:00, 34.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 36/50, Loss: 2.4530270425682392\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 117/117 [00:02<00:00, 44.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 37/50, Loss: 2.3549387597630167\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 117/117 [00:02<00:00, 44.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 38/50, Loss: 2.2583837121979804\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 117/117 [00:02<00:00, 45.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 39/50, Loss: 2.1642361665383363\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 117/117 [00:02<00:00, 43.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 40/50, Loss: 2.072329296006097\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 117/117 [00:03<00:00, 35.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 41/50, Loss: 1.9825822980994852\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 117/117 [00:02<00:00, 44.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 42/50, Loss: 1.8953563117573404\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 117/117 [00:02<00:00, 47.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 43/50, Loss: 1.8106077634371245\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 117/117 [00:02<00:00, 45.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 44/50, Loss: 1.7284525266060462\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 117/117 [00:02<00:00, 39.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 45/50, Loss: 1.6488786925617447\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 117/117 [00:03<00:00, 38.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 46/50, Loss: 1.5718127448334653\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 117/117 [00:02<00:00, 46.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 47/50, Loss: 1.4974153469770382\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 117/117 [00:02<00:00, 45.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 48/50, Loss: 1.4258655849685016\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 117/117 [00:02<00:00, 45.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 49/50, Loss: 1.356891421171335\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 117/117 [00:03<00:00, 38.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 50/50, Loss: 1.290861964225769\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def get_embedding(word, model, word_to_index_mapping):\n",
        "    \"\"\"\n",
        "    Method to get the embedding vector for a given word.\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    word : String\n",
        "        Word for which embedding is required.\n",
        "    model : nn.Module\n",
        "        Trained CBOW model containing the embedding layer.\n",
        "    word_to_index_mapping : Dictionary\n",
        "        Dictionary mapping words to index with format {word: index}.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    word_embedding : Tensor\n",
        "        Embedding vector for the given word.\n",
        "    \"\"\"\n",
        "    # Get the index for the given word, or use <OOV> if word not found\n",
        "    index = word_to_index_mapping.get(word, word_to_index_mapping.get('<OOV>'))\n",
        "\n",
        "    # Ensure inference mode (no gradient calculation)\n",
        "    with torch.no_grad():\n",
        "        # Access the embedding layer weights\n",
        "        embedding_weights = model.embedding.weight\n",
        "        embedding_weights.requires_grad = False  # Ensure no gradient calculation\n",
        "\n",
        "        # Extract the embedding vector for the given word index\n",
        "        word_embedding = embedding_weights[index]\n",
        "\n",
        "    return word_embedding\n",
        "\n",
        "# Test the function with your dataset by getting the embedding of \"capital\"\n",
        "word_embedding = get_embedding(\"capital\", model, word_to_index_mapping)\n",
        "\n",
        "# Print the resulting embedding\n",
        "print(f\"Embedding for 'capital': {word_embedding}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i2SJt9c7MS7X",
        "outputId": "734c23e3-cc45-4aa1-b852-f8971ca016e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding for 'capital': tensor([ 8.2835e-01, -3.9596e-01, -1.2453e+00, -3.1056e-01, -9.2139e-01,\n",
            "         1.4614e-01,  3.0492e-02,  3.4181e-01, -1.7110e+00,  1.2635e+00,\n",
            "         5.2025e-01,  3.8489e-01,  8.3004e-01,  7.6644e-01,  1.2155e+00,\n",
            "        -1.2502e+00,  3.2665e-01, -1.2754e+00, -3.5030e-01, -2.9372e-01,\n",
            "        -1.4092e+00, -1.4293e+00,  7.5238e-01,  4.3461e-01, -1.6176e+00,\n",
            "         3.6742e-01,  3.3168e-01,  3.5906e-01, -8.9272e-01, -2.9902e+00,\n",
            "         6.7162e-01, -9.9007e-01,  5.6658e-01, -1.8158e+00,  8.5976e-01,\n",
            "        -2.3137e-01, -2.0013e-01,  1.6528e+00, -4.9026e-01,  4.8632e-01,\n",
            "        -1.2964e+00,  2.7427e-01, -1.3382e+00, -7.2405e-01, -1.1498e+00,\n",
            "         2.0763e+00, -2.0316e-01,  3.1861e-01, -8.8944e-01, -1.4107e+00,\n",
            "        -8.4982e-01, -1.2443e-01,  1.8189e+00, -1.4448e+00, -2.1442e+00,\n",
            "         3.0062e-01,  8.0135e-01,  1.6293e+00,  4.7966e-02, -4.1063e-01,\n",
            "         2.0211e+00, -8.8137e-01,  4.0413e-01, -3.6083e-01,  1.9096e-01,\n",
            "         9.0848e-01,  8.3992e-03,  5.5238e-01,  9.7174e-02,  7.0644e-01,\n",
            "        -8.1893e-01, -6.1871e-01, -6.2708e-01, -6.9988e-01,  1.5333e+00,\n",
            "        -3.8195e-01, -4.5583e-01,  7.9987e-01,  2.5576e-01,  1.6929e-02,\n",
            "        -2.0280e-01, -1.0813e+00, -9.1308e-01, -1.7671e-02, -4.9165e-01,\n",
            "        -6.3652e-01,  1.6651e-01, -7.2188e-01,  2.0272e-01,  8.3616e-01,\n",
            "        -6.7629e-01, -8.1904e-01, -4.2684e-01, -5.2741e-01,  5.6946e-01,\n",
            "         6.5215e-01, -4.4484e-01,  1.2304e+00,  1.1615e+00,  1.6749e+00,\n",
            "        -1.9869e+00,  1.3016e+00,  7.2541e-01, -4.0349e-01,  1.3702e-01,\n",
            "        -4.2863e-01,  8.3076e-02, -1.7280e+00, -7.7096e-01,  8.3560e-01,\n",
            "        -1.4221e-01,  2.2052e-01, -9.7279e-01,  4.4787e-01,  2.4294e+00,\n",
            "        -4.7165e-01,  5.8114e-01, -4.6568e-01,  4.6929e-01, -2.7899e-01,\n",
            "         3.6654e-01,  2.2736e-01,  4.3117e-01, -1.3464e+00,  1.2160e+00,\n",
            "        -1.0816e+00,  1.8240e-01,  1.5135e+00, -2.0081e+00,  9.5782e-02,\n",
            "         5.1047e-01,  1.5798e-02,  1.8944e+00,  8.0252e-01,  1.6275e+00,\n",
            "        -1.3795e+00,  1.4159e-01,  8.4130e-01,  3.8432e-01,  1.7675e-01,\n",
            "        -5.4537e-01, -8.7606e-01,  2.1719e+00, -7.4977e-01, -7.0519e-01,\n",
            "        -3.9044e-01,  4.0838e-01,  1.2266e+00, -1.1389e+00, -3.9190e-01,\n",
            "         3.1738e-02, -4.6268e-01,  1.2019e+00, -5.5649e-01, -2.1924e+00,\n",
            "        -2.1986e-01,  6.6772e-01,  1.8050e-03,  3.0294e-01, -1.8910e+00,\n",
            "         1.7079e-01,  2.0413e-03, -6.3183e-01, -1.3691e+00, -9.0710e-01,\n",
            "        -6.9611e-01,  7.9087e-01,  1.1263e-01, -6.8908e-01, -1.9977e-01,\n",
            "        -2.5817e-01, -1.3268e+00, -6.0776e-01, -1.2925e+00, -5.6548e-01,\n",
            "         8.2358e-01,  6.7804e-01, -6.9639e-01, -3.7305e-01, -1.4019e+00,\n",
            "        -4.1455e-01, -1.2618e-01, -2.9722e-01, -7.0652e-01,  8.8253e-01,\n",
            "         2.3798e+00,  4.4894e-01, -2.0497e-01, -1.2389e+00,  4.1831e-01,\n",
            "        -2.6905e-01, -6.9948e-01, -7.2528e-01,  6.2408e-01, -1.0763e+00,\n",
            "         3.7800e-01, -1.4241e+00, -3.6248e-02,  6.5200e-01,  4.2718e-01,\n",
            "        -7.5062e-01, -6.2167e-01,  1.9878e-01, -9.6497e-02,  5.4652e-01,\n",
            "         1.2223e+00,  1.2972e-02, -1.8721e+00, -1.3015e+00,  7.2211e-01,\n",
            "         4.4992e-01, -3.6703e-01, -2.6922e-01, -1.1379e+00, -1.1860e+00,\n",
            "         1.4967e-01,  1.5902e+00, -2.1675e+00,  1.3823e+00, -4.6602e-01,\n",
            "         3.2283e-01,  9.8644e-01, -3.4512e-01, -9.3947e-01,  1.8071e+00,\n",
            "         1.1064e-02, -5.2865e-01,  4.4772e-01,  4.2226e-02, -1.6980e+00,\n",
            "        -8.6495e-01, -1.1756e+00,  1.1109e+00, -4.1864e-01, -4.6012e-01,\n",
            "        -8.1537e-01, -1.7352e+00, -1.1894e+00,  4.7986e-01, -4.1543e-02,\n",
            "        -1.0447e-01,  9.0387e-01,  8.5028e-04,  2.2643e+00,  3.6312e-01,\n",
            "         9.9156e-01, -5.9605e-01,  4.7852e-01,  1.2918e+00,  3.5091e-01,\n",
            "        -1.0752e+00, -3.1419e-01, -7.0816e-01,  1.0159e-01, -1.2006e+00,\n",
            "         1.5376e+00,  3.1326e-01,  9.5936e-01,  5.8693e-01,  5.9061e-01,\n",
            "         1.3598e-01,  5.0198e-01, -2.8008e-01,  3.7392e-01, -1.0138e+00,\n",
            "        -2.2678e-01, -2.6973e-01, -8.2170e-01,  9.6083e-01, -3.2321e-01,\n",
            "         1.8677e-01,  2.0253e+00, -1.6191e-02,  9.1891e-01, -3.2467e-01,\n",
            "         1.0104e+00,  7.1554e-01,  9.1138e-01,  6.2666e-01, -1.0129e-01,\n",
            "         7.8975e-03,  1.0483e+00,  9.6863e-01, -1.0229e+00, -1.3966e+00,\n",
            "         9.3049e-01,  2.3424e-01, -1.4449e+00, -3.5659e-01,  1.1844e+00,\n",
            "        -1.2112e-01,  1.0119e+00,  4.4557e-01, -1.7083e+00, -3.2308e-01,\n",
            "         2.2198e-01, -5.4477e-01, -2.3913e+00, -1.5533e-02,  2.2146e+00])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def cosine_similarity(v1, v2):\n",
        "    \"\"\"\n",
        "    Method to calculate cosine similarity between two vectors.\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    v1 : Tensor\n",
        "        First vector.\n",
        "    v2 : Tensor\n",
        "        Second vector.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    cosine_sim : float\n",
        "        Cosine similarity between v1 and v2.\n",
        "    \"\"\"\n",
        "    dot_product = torch.dot(v1, v2)  # Calculate the dot product\n",
        "    v1_norm = torch.norm(v1)  # Calculate the norm of v1\n",
        "    v2_norm = torch.norm(v2)  # Calculate the norm of v2\n",
        "\n",
        "    # Avoid division by zero\n",
        "    if v1_norm == 0 or v2_norm == 0:\n",
        "        return 0.0\n",
        "\n",
        "    cosine_sim = dot_product / (v1_norm * v2_norm)  # Compute cosine similarity\n",
        "    return cosine_sim.item()  # Convert to Python float\n",
        "\n",
        "# Test the cosine similarity between two words using your CBOW model\n",
        "def test_cosine_similarity(word1, word2, model, word_to_index_mapping):\n",
        "    \"\"\"\n",
        "    Method to test cosine similarity between the embeddings of two words.\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    word1, word2 : String\n",
        "        Words to compare.\n",
        "    model : nn.Module\n",
        "        Trained CBOW model.\n",
        "    word_to_index_mapping : Dictionary\n",
        "        Dictionary mapping words to indices.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    similarity : float\n",
        "        Cosine similarity between the embeddings of the two words.\n",
        "    \"\"\"\n",
        "    # Get the embeddings for both words\n",
        "    embedding1 = get_embedding(word1, model, word_to_index_mapping)\n",
        "    embedding2 = get_embedding(word2, model, word_to_index_mapping)\n",
        "\n",
        "    # Calculate and return the cosine similarity\n",
        "    similarity = cosine_similarity(embedding1, embedding2)\n",
        "    return similarity\n",
        "\n",
        "# Example usage\n",
        "word1 = \"capital\"\n",
        "word2 = \"state\"\n",
        "\n",
        "similarity = test_cosine_similarity(word1, word2, model, word_to_index_mapping)\n",
        "print(f\"Cosine similarity between '{word1}' and '{word2}': {similarity}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odXsA_9tMkX-",
        "outputId": "5ac34da6-198b-473b-9b76-85aba10baced"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine similarity between 'capital' and 'state': 0.15685120224952698\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def get_k_nearest_words(k, word, vocabulary, model, word_to_index):\n",
        "    \"\"\"\n",
        "    Method to find the k nearest words of a given word.\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    k : int\n",
        "        Number of nearest words to return.\n",
        "    word : String\n",
        "        Word for which to find the k nearest words.\n",
        "    vocabulary : Dictionary\n",
        "        Dictionary mapping words to their frequency (or presence) {word: frequency}.\n",
        "    model : nn.Module\n",
        "        Trained CBOW model.\n",
        "    word_to_index : Dictionary\n",
        "        Dictionary mapping words to index {word: index}.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    similar : List of Strings\n",
        "        List of k nearest words to the given word.\n",
        "    \"\"\"\n",
        "\n",
        "    # Get the embedding for the given word using the model\n",
        "    word_embedding = get_embedding(word, model, word_to_index)\n",
        "\n",
        "    # Initialize a tensor to store similarity scores (1 score per word in the vocabulary)\n",
        "    similarity_scores = torch.zeros(len(vocabulary))\n",
        "\n",
        "    # Get a list of vocabulary keys once (for efficiency)\n",
        "    vocab_keys = list(vocabulary.keys())\n",
        "\n",
        "    # Iterate over all words in the vocabulary to compute similarities\n",
        "    for i, other_word in enumerate(vocab_keys):\n",
        "        # Skip if the word is the same as the input word\n",
        "        if other_word == word:\n",
        "            similarity_scores[i] = -float('inf')  # Assign a very low score to skip\n",
        "            continue\n",
        "\n",
        "        # Get the embedding of the other word\n",
        "        other_word_embedding = get_embedding(other_word, model, word_to_index)\n",
        "\n",
        "        # Compute cosine similarity between the word and the other word\n",
        "        similarity_scores[i] = cosine_similarity(word_embedding, other_word_embedding)\n",
        "\n",
        "    # Get the indices of the top k most similar words using torch.topk\n",
        "    k_first = torch.topk(similarity_scores, k)\n",
        "\n",
        "    # Extract the indices of the k most similar words\n",
        "    similar_word_indices = k_first.indices\n",
        "\n",
        "    # Convert the indices back to words from the vocabulary\n",
        "    similar = [vocab_keys[index] for index in similar_word_indices]\n",
        "\n",
        "    return similar\n",
        "\n",
        "# Example usage:\n",
        "similar_words = get_k_nearest_words(5, \"capital\", vocabulary, model, word_to_index_mapping)\n",
        "print(f\"Top 5 words similar to 'capital': {similar_words}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJpunRHDM-Ud",
        "outputId": "5729800a-0e54-4979-a1ed-80c015d69aae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 words similar to 'capital': ['seat', 'houses', 'end', 'rest', 'cordilleran']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "def test_analogy(model, word_to_index_mapping, analogy_file, subset_size=None):\n",
        "    \"\"\"\n",
        "    Method to test accuracy of CBOW embeddings on analogy tasks.\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    model : nn.Module\n",
        "        Trained CBOW model.\n",
        "    word_to_index_mapping : Dictionary\n",
        "        Dictionary mapping words to indices {word: index}.\n",
        "    analogy_file : String\n",
        "        File containing analogy tasks.\n",
        "    subset_size : int, optional\n",
        "        Number of rows to use from the dataset (for testing purposes).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    accuracy : float\n",
        "        Accuracy of the model on the analogy tasks.\n",
        "    \"\"\"\n",
        "    # Load the CSV file\n",
        "    df = pd.read_csv(analogy_file)\n",
        "\n",
        "    # Optional: Reduce dataset size for quick testing\n",
        "    if subset_size:\n",
        "        df = df.head(subset_size)\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    skipped = 0  # Counter for skipped tasks\n",
        "\n",
        "    # Iterate through each analogy task\n",
        "    for index, row in df.iterrows():\n",
        "        word_one = row['word_one'].lower()\n",
        "        word_two = row['word_two'].lower()\n",
        "        word_three = row['word_three'].lower()\n",
        "        word_four = row['word_four'].lower()\n",
        "\n",
        "        # Skip tasks if any word is not in the vocabulary\n",
        "        if (word_one not in word_to_index_mapping) or \\\n",
        "           (word_two not in word_to_index_mapping) or \\\n",
        "           (word_three not in word_to_index_mapping) or \\\n",
        "           (word_four not in word_to_index_mapping):\n",
        "            skipped += 1\n",
        "            continue\n",
        "\n",
        "        # Get embeddings for the words\n",
        "        embedding_word_one = get_embedding(word_one, model, word_to_index_mapping)\n",
        "        embedding_word_two = get_embedding(word_two, model, word_to_index_mapping)\n",
        "        embedding_word_three = get_embedding(word_three, model, word_to_index_mapping)\n",
        "\n",
        "        # Compute the analogy vector\n",
        "        analogy_vector = embedding_word_two - embedding_word_one + embedding_word_three\n",
        "\n",
        "        # Get the top 3 nearest words to the analogy vector\n",
        "        predictions = get_k_nearest_words(3, word_three, vocabulary, model, word_to_index_mapping)\n",
        "\n",
        "        # Check if the correct word is among the predictions\n",
        "        is_correct = word_four in predictions\n",
        "        print(f\"Analogy: {word_one} -> {word_two} :: {word_three} -> {word_four} | \"\n",
        "              f\"Prediction: {predictions}, Correct: {is_correct}\")\n",
        "\n",
        "        if is_correct:\n",
        "            correct += 1\n",
        "\n",
        "        total += 1\n",
        "\n",
        "    # Print summary stats\n",
        "    print(f\"Total tasks skipped due to missing words: {skipped}\")\n",
        "    if total == 0:\n",
        "        print(\"No valid tasks were processed.\")\n",
        "        return 'No word was found in the embeddings'\n",
        "\n",
        "    accuracy = correct / total\n",
        "    print(f\"Analogy task accuracy: {accuracy:.4f}\")\n",
        "    return accuracy\n",
        "\n",
        "# Ensure the word-to-index mapping is generated from your vocabulary\n",
        "word_to_index_mapping = word_to_index(vocabulary)  # Call function to generate the dictionary\n",
        "\n",
        "# Test the model on your uploaded dataset using the correct dictionary\n",
        "accuracy = test_analogy(model, word_to_index_mapping, 'us_states_capitals_analogy.csv')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NnvZ2lOBObbM",
        "outputId": "4aa90358-0668-40aa-9d73-ab6d6a662f7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analogy: alabama -> montgomery :: hawaii -> honolulu | Prediction: ['states', 'juan', 'intermontane'], Correct: False\n",
            "Analogy: alabama -> montgomery :: massachusetts -> boston | Prediction: ['greater', 'highest', 'boston'], Correct: True\n",
            "Analogy: alabama -> montgomery :: texas -> austin | Prediction: ['deep', 'brown', 'allow'], Correct: False\n",
            "Analogy: alabama -> montgomery :: vermont -> montpelier | Prediction: ['supreme', 'seismic', 'india'], Correct: False\n",
            "Analogy: alabama -> montgomery :: virginia -> richmond | Prediction: ['snowfall', 'lush', 'used'], Correct: False\n",
            "Analogy: hawaii -> honolulu :: massachusetts -> boston | Prediction: ['greater', 'highest', 'boston'], Correct: True\n",
            "Analogy: hawaii -> honolulu :: texas -> austin | Prediction: ['deep', 'brown', 'allow'], Correct: False\n",
            "Analogy: hawaii -> honolulu :: vermont -> montpelier | Prediction: ['supreme', 'seismic', 'india'], Correct: False\n",
            "Analogy: hawaii -> honolulu :: virginia -> richmond | Prediction: ['snowfall', 'lush', 'used'], Correct: False\n",
            "Analogy: massachusetts -> boston :: texas -> austin | Prediction: ['deep', 'brown', 'allow'], Correct: False\n",
            "Analogy: massachusetts -> boston :: vermont -> montpelier | Prediction: ['supreme', 'seismic', 'india'], Correct: False\n",
            "Analogy: massachusetts -> boston :: virginia -> richmond | Prediction: ['snowfall', 'lush', 'used'], Correct: False\n",
            "Analogy: texas -> austin :: vermont -> montpelier | Prediction: ['supreme', 'seismic', 'india'], Correct: False\n",
            "Analogy: texas -> austin :: virginia -> richmond | Prediction: ['snowfall', 'lush', 'used'], Correct: False\n",
            "Analogy: vermont -> montpelier :: virginia -> richmond | Prediction: ['snowfall', 'lush', 'used'], Correct: False\n",
            "Total tasks skipped due to missing words: 1210\n",
            "Analogy task accuracy: 0.1333\n"
          ]
        }
      ]
    }
  ]
}